# Database Migration Guide: SQLite to PostgreSQL

## Overview

This guide describes the complete migration process from SQLite to PostgreSQL for the –í–æ–ª—å—Ç–∏–∫ Telegram Bot v2.0.

## ‚ö†Ô∏è CRITICAL: Zero Data Loss Guarantee

This migration is designed to ensure **ZERO DATA LOSS**. All user settings, channels, IPs, queues, and notification preferences will be preserved.

## Prerequisites

- Node.js 20.0.0 or higher
- PostgreSQL 14 or higher
- Access to current SQLite database (`bot.db`)
- PostgreSQL credentials

## Step-by-Step Migration Process

### 1. Backup Current Database

**CRITICAL: Always create a backup before migration!**

```bash
# Automatic backup (recommended)
npm run migrate:dry-run

# Manual backup
cp data/bot.db backups/bot_backup_$(date +%Y%m%d_%H%M%S).db
```

Backups are stored in `./backups/` with SHA-256 checksums for verification.

### 2. Setup PostgreSQL

#### Option A: Local PostgreSQL

```bash
# Install PostgreSQL (Ubuntu/Debian)
sudo apt-get install postgresql postgresql-contrib

# Create database and user
sudo -u postgres psql
CREATE DATABASE voltik;
CREATE USER voltik WITH ENCRYPTED PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE voltik TO voltik;
\q
```

#### Option B: Railway (Production)

1. Create PostgreSQL database in Railway dashboard
2. Copy connection credentials
3. Set environment variables (see step 3)

### 3. Configure Environment Variables

Create or update `.env` file:

```bash
# Database Type
DB_TYPE=postgres  # Change from 'sqlite' to 'postgres'

# PostgreSQL Connection
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=voltik
POSTGRES_USER=voltik
POSTGRES_PASSWORD=your_secure_password

# Legacy SQLite (for migration)
DATABASE_PATH=./data/bot.db
```

### 4. Install Dependencies

```bash
npm install
```

This will install `pg` (PostgreSQL client) and other required packages.

### 5. Test Migration (Dry Run)

**ALWAYS run dry run first!**

```bash
npm run migrate:dry-run
```

This will:
- Create a backup
- Connect to both databases
- Count records in each table
- Simulate migration without writing to PostgreSQL
- Generate a report

Review the output carefully. Expected output:

```
üîÑ Starting migration from SQLite to PostgreSQL...
üì¶ Creating backup...
‚úÖ Backup created: ./backups/bot_backup_2026-02-06T20-00-00.db
üîê Checksum: a1b2c3d4e5f6...

üß™ Dry run: YES

üë• Migrating users...
üìä Found 150 users
üß™ Dry run - skipping actual migration

üìä Migration Report
==================================================
üì¶ Backup: ./backups/bot_backup_2026-02-06T20-00-00.db
üë• Users to migrate: 150
‚ö° Outage records: 45
üîã Power history: 320
‚öôÔ∏è  Settings: 5
üìÖ Schedule history: 890
üîÑ User states: 23
==================================================
```

### 6. Run Actual Migration

Once dry run looks good:

```bash
npm run migrate
```

This will:
1. Create a timestamped backup with checksum
2. Connect to both databases
3. Migrate all tables in order:
   - users (with all settings, regions, queues)
   - outage_history
   - power_history
   - settings
   - schedule_history
   - user_states
   - pending_channels
4. Verify data integrity
5. Generate migration report

**Migration is transactional** - if any step fails, changes are rolled back.

### 7. Verify Migration

After successful migration:

```bash
# Check user count
psql -U voltik -d voltik -c "SELECT COUNT(*) FROM users;"

# Sample user data
psql -U voltik -d voltik -c "SELECT telegram_id, region, queue, channel_id FROM users LIMIT 10;"

# Check tables
psql -U voltik -d voltik -c "\dt"
```

### 8. Update Application Configuration

Update `.env` to use PostgreSQL:

```bash
DB_TYPE=postgres
```

### 9. Start Bot with PostgreSQL

```bash
npm start
```

Watch the logs for:
```
üêò Using PostgreSQL database
‚úÖ PostgreSQL connection established
‚úÖ Database schema initialized
‚ú® –ë–æ—Ç —É—Å–ø—ñ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–æ —Ç–∞ –≥–æ—Ç–æ–≤–∏–π –¥–æ —Ä–æ–±–æ—Ç–∏!
```

### 10. Verify Bot Functionality

Test the following:

1. **User commands work**: `/start`, `/schedule`, `/settings`
2. **User settings preserved**: Check your region/queue
3. **Channels still connected**: Verify channel publications
4. **IP monitoring active**: Check power state notifications
5. **No errors in logs**: Monitor for 10-15 minutes

## Rollback Procedure

If anything goes wrong:

### Option 1: Quick Rollback (Use SQLite Backup)

```bash
# Stop the bot
pm2 stop voltik  # or Ctrl+C

# Restore from backup
cp backups/bot_backup_TIMESTAMP.db data/bot.db

# Verify checksum
sha256sum data/bot.db
cat backups/bot_backup_TIMESTAMP.db.sha256

# Update .env
DB_TYPE=sqlite

# Restart bot
npm start
```

### Option 2: Re-run Migration

If PostgreSQL data is corrupted but SQLite backup is good:

```bash
# Drop PostgreSQL tables
psql -U voltik -d voltik -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;"

# Re-run migration
npm run migrate
```

## Migration Reports

Migration reports are saved in `./backups/`:

- `bot_backup_TIMESTAMP.db` - SQLite backup
- `bot_backup_TIMESTAMP.db.sha256` - Backup checksum
- `migration_report_TIMESTAMP.json` - Migration statistics

Example report:

```json
{
  "timestamp": "2026-02-06T20:00:00.000Z",
  "backup": {
    "backupPath": "./backups/bot_backup_2026-02-06T20-00-00.db",
    "checksum": "a1b2c3d4..."
  },
  "stats": {
    "users": 150,
    "outages": 45,
    "powerHistory": 320,
    "settings": 5,
    "scheduleHistory": 890,
    "userStates": 23
  },
  "success": true
}
```

## Troubleshooting

### Migration fails with "User not found"

**Cause**: Foreign key references are checked strictly in PostgreSQL.

**Fix**: Migration script handles this automatically by skipping orphaned records and logging warnings.

### Connection timeout to PostgreSQL

**Cause**: Network issues or wrong credentials.

**Fix**:
1. Verify PostgreSQL is running: `pg_isready`
2. Test connection: `psql -U voltik -d voltik`
3. Check firewall rules
4. Verify credentials in `.env`

### "ENOENT: no such file or directory" for SQLite

**Cause**: No SQLite database exists (fresh install).

**Fix**: This is expected for new installations. Skip migration and proceed with PostgreSQL directly.

### High memory usage during migration

**Cause**: Large datasets (>10,000 users).

**Fix**: Migration processes records in batches. For very large datasets, contact the maintainer for optimized migration script.

## Post-Migration Checklist

- [ ] Backup created and checksum verified
- [ ] Migration completed without errors
- [ ] User count matches between SQLite and PostgreSQL
- [ ] Sample users verified (data integrity)
- [ ] Bot started with PostgreSQL
- [ ] All commands work correctly
- [ ] Channels still publishing
- [ ] IP monitoring functional
- [ ] No errors in logs for 15+ minutes
- [ ] Backup files archived safely

## Data Retention

After successful migration:

- **Keep SQLite backup** for at least 30 days
- **Archive migration reports** permanently
- **Monitor PostgreSQL** for performance and errors
- **Setup automated PostgreSQL backups** (pg_dump)

## Support

For migration issues:
1. Check logs in `./backups/migration_report_*.json`
2. Review error messages
3. Attempt rollback if necessary
4. Contact repository maintainer with migration report

## Schema Differences

### SQLite ‚Üí PostgreSQL Mappings

| SQLite | PostgreSQL | Notes |
|--------|------------|-------|
| `INTEGER` | `SERIAL` / `BIGINT` | Auto-increment IDs |
| `TEXT` | `TEXT` | No changes |
| `BOOLEAN` | `BOOLEAN` | Native boolean type |
| `DATETIME` | `TIMESTAMP WITH TIME ZONE` | Timezone-aware |
| JSON in TEXT | `JSONB` | Native JSON with indexing |

### New Tables in v2.0

- `publication_signatures` - For idempotency
- `pause_log` - Track notification pauses

### Enhanced Indexes

PostgreSQL schema includes optimized indexes for:
- User lookups by telegram_id
- Region/queue filtering
- Power state queries
- Schedule history searches
- Publication signature checks

## Performance Expectations

| Metric | SQLite | PostgreSQL |
|--------|--------|------------|
| User lookup | ~1ms | ~0.5ms |
| Concurrent writes | Limited | Excellent |
| Full table scan (10k users) | ~50ms | ~20ms |
| Index performance | Good | Excellent |
| Connection pooling | No | Yes (20 connections) |

PostgreSQL will provide better performance, especially for:
- Concurrent users
- Channel publications
- IP monitoring at scale
- Complex queries

---

**Remember**: Take your time, always backup first, and verify each step!

‚ö° Migration designed for ZERO data loss
üîê All backups include checksums
‚úÖ Transactional migration with rollback support
